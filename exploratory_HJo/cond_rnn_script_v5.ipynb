{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from utility import *\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "from utility_HJo import *  # defines concatDF, dataPrep function, etc\n",
    "from utility_HJo_condRNN import *  # defines functions related to condRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\OneDrive - California Institute of Technology\\class\\COVID-19\\SKTW\n"
     ]
    }
   ],
   "source": [
    "homedir = get_homedir(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPS_mapping, _ = get_FIPS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve berkeley dataset for demogaphical data\n",
    "df_berk_orig = pd.read_csv(\"../data/us/aggregate_berkeley.csv\")\n",
    "df_berk_orig[\"countyFIPS\"]=df_berk_orig[\"countyFIPS\"].apply(correct_FIPS)\n",
    "df_berk = fix_FIPS(df_berk_orig, fipslabel=\"countyFIPS\")\n",
    "df_berk.set_index(['countyFIPS'], inplace = True)\n",
    "df_berk_orig.set_index(['countyFIPS'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_berk_orig.columns = df_berk_orig.columns.str.replace(\"'\",\" \")\n",
    "df_berk.columns = df_berk.columns.str.replace(\"'\",\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geolocation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve geolocation data\n",
    "df_geo = pd.read_csv(\"../data/us/geolocation/county_centers.csv\")\n",
    "df_geo[\"fips\"]=df_geo[\"fips\"].apply(correct_FIPS)\n",
    "df_geo = fix_FIPS(df_geo, fipslabel=\"fips\", reduced=True)\n",
    "df_geo.set_index(['fips'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add population-weighted center location of each county to df_berk\n",
    "df_geo_temp = df_geo[[\"pclon10\", \"pclat10\"]]\n",
    "df_berk2 = pd.concat([df_berk, df_geo_temp.reindex(df_berk.index)], axis = 1, sort=False, join='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'PopulationEstimate2018', 'Population(Persons)2017',\n",
       "       'PopTotalMale2017', 'PopTotalFemale2017', 'FracMale2017',\n",
       "       'PopulationEstimate65+2017', 'PopulationDensityperSqMile2010',\n",
       "       'CensusPopulation2010', 'MedianAge2010', 'MedianAge,Male2010',\n",
       "       'MedianAge,Female2010', '#EligibleforMedicare2018',\n",
       "       'MedicareEnrollment,AgedTot2017', '3-YrDiabetes2015-17',\n",
       "       'DiabetesPercentage', 'HeartDiseaseMortality', 'StrokeMortality',\n",
       "       'Smokers_Percentage', '#FTEHospitalTotal2017',\n",
       "       'TotalM.D. s,TotNon-FedandFed2017', '#HospParticipatinginNetwork2017',\n",
       "       '#Hospitals', '#ICU_beds', 'dem_to_rep_ratio', 'PopMale<52010',\n",
       "       'PopFmle<52010', 'PopMale5-92010', 'PopFmle5-92010', 'PopMale10-142010',\n",
       "       'PopFmle10-142010', 'PopMale15-192010', 'PopFmle15-192010',\n",
       "       'PopMale20-242010', 'PopFmle20-242010', 'PopMale25-292010',\n",
       "       'PopFmle25-292010', 'PopMale30-342010', 'PopFmle30-342010',\n",
       "       'PopMale35-442010', 'PopFmle35-442010', 'PopMale45-542010',\n",
       "       'PopFmle45-542010', 'PopMale55-592010', 'PopFmle55-592010',\n",
       "       'PopMale60-642010', 'PopFmle60-642010', 'PopMale65-742010',\n",
       "       'PopFmle65-742010', 'PopMale75-842010', 'PopFmle75-842010',\n",
       "       'PopMale>842010', 'PopFmle>842010', '3-YrMortalityAge<1Year2015-17',\n",
       "       '3-YrMortalityAge1-4Years2015-17', '3-YrMortalityAge5-14Years2015-17',\n",
       "       '3-YrMortalityAge15-24Years2015-17',\n",
       "       '3-YrMortalityAge25-34Years2015-17',\n",
       "       '3-YrMortalityAge35-44Years2015-17',\n",
       "       '3-YrMortalityAge45-54Years2015-17',\n",
       "       '3-YrMortalityAge55-64Years2015-17',\n",
       "       '3-YrMortalityAge65-74Years2015-17',\n",
       "       '3-YrMortalityAge75-84Years2015-17', '3-YrMortalityAge85+Years2015-17',\n",
       "       'mortality2015-17Estimated', 'pclon10', 'pclat10', 'GDP_2015',\n",
       "       'GDP_2016', 'GDP_2017', 'GDP_2018'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GDP = pd.read_csv(\"../JK/GDP.csv\")\n",
    "df_GDP[\"fips\"]=df_GDP[\"fips\"].apply(correct_FIPS)\n",
    "df_GDP = fix_FIPS(df_GDP, fipslabel=\"fips\", reduced=True)\n",
    "df_GDP.set_index(['fips'], inplace = True)\n",
    "# change column names\n",
    "df_GDP.rename(columns={\"2015\": \"GDP_2015\", \"2016\": \"GDP_2016\", \"2017\": \"GDP_2017\", \"2018\": \"GDP_2018\"},  inplace = True)\n",
    "#df_GDP.head()\n",
    "\n",
    "df_berk2 = pd.concat([df_berk2, df_GDP.reindex(df_berk.index)], axis = 1, sort=False, join='outer')\n",
    "df_berk2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster data from Juhyun's work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"\\exploratory_HJo\\n_clusters=5_kmeans_extended.txt\"\n",
    "with open(f'{homedir}'+path, 'r') as f:\n",
    "    dic_cluster = eval(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mortality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['01001', '01003', '01005', '01007', '01009', '01011', '01013', '01015',\n",
      "       '01017', '01019',\n",
      "       ...\n",
      "       '56025', '56027', '56029', '56031', '56033', '56035', '56037', '56039',\n",
      "       '56041', '56043'],\n",
      "      dtype='object', name='fips', length=2927)\n"
     ]
    }
   ],
   "source": [
    "# retrieve nyt mortality data for the values to be predicted\n",
    "# df_nyt = pd.read_csv(\"../data/us/covid/nyt_us_counties_daily.csv\", index_col=[0,1])\n",
    "df_nyt_orig = pd.read_csv(\"../data/us/covid/nyt_us_counties.csv\") # , index_col=[0,3]      0: date, 3: fips\n",
    "df_nyt_orig.dropna(subset = [\"fips\"], inplace = True)\n",
    "df_nyt_orig['weekday'] = df_nyt_orig['date']\n",
    "df_nyt_orig['weekday'] = pd.to_datetime(df_nyt_orig['weekday'], format = \"%Y-%m-%d\")\n",
    "df_nyt_orig['weekday'] = df_nyt_orig['weekday'].dt.dayofweek\n",
    "df_nyt_orig[\"fips\"]=df_nyt_orig[\"fips\"].apply(correct_FIPS)\n",
    "df_nyt = fix_FIPS(df_nyt_orig, fipslabel=\"fips\", datelabel = \"date\", reduced=True)\n",
    "\n",
    "df_nyt.set_index(['date', 'fips'], inplace = True)\n",
    "\n",
    "dateList = df_nyt.index.levels[0]\n",
    "fipsList = df_nyt.index.levels[1]\n",
    "print(fipsList)\n",
    "\n",
    "# define separate df for each of the counties\n",
    "df_nyt_eachCounty_accum = [df_nyt.xs(fips, level = 'fips') for fips in fipsList]\n",
    "df_nyt_eachCounty = deepcopy(df_nyt_eachCounty_accum)\n",
    "for i in range(len(df_nyt_eachCounty)):\n",
    "    df_nyt_eachCounty[i].deaths = np.diff(np.concatenate(([0], df_nyt_eachCounty[i].loc[:, 'deaths'])) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for converting fips into state\n",
    "df_fips2state = df_nyt_orig[['state', 'fips']].copy()\n",
    "df_fips2state.drop_duplicates(inplace = True)\n",
    "df_fips2state.set_index('fips', inplace = True)\n",
    "#print(df_fips2state.loc['02050', 'state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobility data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjfips</th>\n",
       "      <th>instate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orgfips</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>01001</td>\n",
       "      <td>01021</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01001</td>\n",
       "      <td>01047</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01001</td>\n",
       "      <td>01051</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01001</td>\n",
       "      <td>01085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01001</td>\n",
       "      <td>01101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        adjfips  instate\n",
       "orgfips                 \n",
       "01001     01021        1\n",
       "01001     01047        1\n",
       "01001     01051        1\n",
       "01001     01085        1\n",
       "01001     01101        1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get neighboring county data so that we can fill the missing data in mobility dataset\n",
    "df_neighbor = pd.read_csv(\"../data/us/geolocation/neighborcounties.csv\")\n",
    "df_neighbor[\"orgfips\"]=df_neighbor[\"orgfips\"].apply(correct_FIPS)\n",
    "df_neighbor[\"adjfips\"]=df_neighbor[\"adjfips\"].apply(correct_FIPS)\n",
    "df_neighbor.set_index(['orgfips'], inplace = True)\n",
    "df_neighbor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mobility_orig = pd.read_csv('../data/us/mobility/DL-us-mobility-daterow.csv')\n",
    "df_mobility_orig = df_mobility_orig[~np.isnan(df_mobility_orig.fips)]\n",
    "df_mobility_orig[\"fips\"]=df_mobility_orig[\"fips\"].apply(correct_FIPS)\n",
    "df_mobility = fix_FIPS(df_mobility_orig, fipslabel=\"fips\", datelabel = \"date\", reduced=True)\n",
    "df_mobility.set_index(['date', 'fips'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fips  02050 not found - using state-wise mobility data\n",
      "fips  02150 not found - using state-wise mobility data\n",
      "fips  02180 not found - using state-wise mobility data\n",
      "fips  02188 not found - using state-wise mobility data\n",
      "fips  02195 not found - using state-wise mobility data\n",
      "fips  02201 not found - using state-wise mobility data\n",
      "fips  02220 not found - using state-wise mobility data\n",
      "fips  02240 not found - using state-wise mobility data\n",
      "fips  02261 not found - using state-wise mobility data\n",
      "fips  02290 not found - using state-wise mobility data\n",
      "fips  05049 not found - using state-wise mobility data\n",
      "fips  05101 not found - using state-wise mobility data\n",
      "fips  06003 not found - using state-wise mobility data\n",
      "fips  06105 not found - using state-wise mobility data\n",
      "fips  08009 not found - using state-wise mobility data\n",
      "fips  08011 not found - using state-wise mobility data\n",
      "fips  08017 not found - using state-wise mobility data\n",
      "fips  08021 not found - using state-wise mobility data\n",
      "fips  08023 not found - using state-wise mobility data\n",
      "fips  08025 not found - using state-wise mobility data\n",
      "fips  08027 not found - using state-wise mobility data\n",
      "fips  08053 not found - using state-wise mobility data\n",
      "fips  08055 not found - using state-wise mobility data\n",
      "fips  08065 not found - using state-wise mobility data\n",
      "fips  08079 not found - using state-wise mobility data\n",
      "fips  08091 not found - using state-wise mobility data\n",
      "fips  08095 not found - using state-wise mobility data\n",
      "fips  08103 not found - using state-wise mobility data\n",
      "fips  08109 not found - using state-wise mobility data\n",
      "fips  08111 not found - using state-wise mobility data\n",
      "fips  08121 not found - using state-wise mobility data\n",
      "fips  11001 not found - using state-wise mobility data\n",
      "fips  13007 not found - using state-wise mobility data\n",
      "fips  13037 not found - using state-wise mobility data\n",
      "fips  13061 not found - using state-wise mobility data\n",
      "fips  13125 not found - using state-wise mobility data\n",
      "fips  13141 not found - using state-wise mobility data\n",
      "fips  13239 not found - using state-wise mobility data\n",
      "fips  13259 not found - using state-wise mobility data\n",
      "fips  13265 not found - using state-wise mobility data\n",
      "fips  13301 not found - using state-wise mobility data\n",
      "fips  13307 not found - using state-wise mobility data\n",
      "fips  13309 not found - using state-wise mobility data\n",
      "fips  16003 not found - using state-wise mobility data\n",
      "fips  16025 not found - using state-wise mobility data\n",
      "fips  16037 not found - using state-wise mobility data\n",
      "fips  16049 not found - using state-wise mobility data\n",
      "fips  16059 not found - using state-wise mobility data\n",
      "fips  16063 not found - using state-wise mobility data\n",
      "fips  17003 not found - using state-wise mobility data\n",
      "fips  17013 not found - using state-wise mobility data\n",
      "fips  17059 not found - using state-wise mobility data\n",
      "fips  17069 not found - using state-wise mobility data\n",
      "fips  17071 not found - using state-wise mobility data\n",
      "fips  17151 not found - using state-wise mobility data\n",
      "fips  17153 not found - using state-wise mobility data\n",
      "fips  17175 not found - using state-wise mobility data\n",
      "fips  19003 not found - using state-wise mobility data\n",
      "fips  19009 not found - using state-wise mobility data\n",
      "fips  19119 not found - using state-wise mobility data\n",
      "fips  19143 not found - using state-wise mobility data\n",
      "fips  19159 not found - using state-wise mobility data\n",
      "fips  19177 not found - using state-wise mobility data\n",
      "fips  19185 not found - using state-wise mobility data\n",
      "fips  20007 not found - using state-wise mobility data\n",
      "fips  20017 not found - using state-wise mobility data\n",
      "fips  20019 not found - using state-wise mobility data\n",
      "fips  20023 not found - using state-wise mobility data\n",
      "fips  20025 not found - using state-wise mobility data\n",
      "fips  20047 not found - using state-wise mobility data\n",
      "fips  20063 not found - using state-wise mobility data\n",
      "fips  20067 not found - using state-wise mobility data\n",
      "fips  20073 not found - using state-wise mobility data\n",
      "fips  20075 not found - using state-wise mobility data\n",
      "fips  20081 not found - using state-wise mobility data\n",
      "fips  20089 not found - using state-wise mobility data\n",
      "fips  20093 not found - using state-wise mobility data\n",
      "fips  20097 not found - using state-wise mobility data\n",
      "fips  20119 not found - using state-wise mobility data\n",
      "fips  20129 not found - using state-wise mobility data\n",
      "fips  20135 not found - using state-wise mobility data\n",
      "fips  20137 not found - using state-wise mobility data\n",
      "fips  20141 not found - using state-wise mobility data\n",
      "fips  20147 not found - using state-wise mobility data\n",
      "fips  20157 not found - using state-wise mobility data\n",
      "fips  20163 not found - using state-wise mobility data\n",
      "fips  20179 not found - using state-wise mobility data\n",
      "fips  20183 not found - using state-wise mobility data\n",
      "fips  20185 not found - using state-wise mobility data\n",
      "fips  20187 not found - using state-wise mobility data\n",
      "fips  20197 not found - using state-wise mobility data\n",
      "fips  20203 not found - using state-wise mobility data\n",
      "fips  20207 not found - using state-wise mobility data\n",
      "fips  21039 not found - using state-wise mobility data\n",
      "fips  21063 not found - using state-wise mobility data\n",
      "fips  21105 not found - using state-wise mobility data\n",
      "fips  21165 not found - using state-wise mobility data\n",
      "fips  21189 not found - using state-wise mobility data\n",
      "fips  21237 not found - using state-wise mobility data\n",
      "fips  26013 not found - using state-wise mobility data\n",
      "fips  26095 not found - using state-wise mobility data\n",
      "fips  26131 not found - using state-wise mobility data\n",
      "fips  26135 not found - using state-wise mobility data\n",
      "fips  26141 not found - using state-wise mobility data\n",
      "fips  26153 not found - using state-wise mobility data\n",
      "fips  27011 not found - using state-wise mobility data\n",
      "fips  27029 not found - using state-wise mobility data\n",
      "fips  27069 not found - using state-wise mobility data\n",
      "fips  27073 not found - using state-wise mobility data\n",
      "fips  27081 not found - using state-wise mobility data\n",
      "fips  27087 not found - using state-wise mobility data\n",
      "fips  27089 not found - using state-wise mobility data\n",
      "fips  27101 not found - using state-wise mobility data\n",
      "fips  27107 not found - using state-wise mobility data\n",
      "fips  27125 not found - using state-wise mobility data\n",
      "fips  27155 not found - using state-wise mobility data\n",
      "fips  27167 not found - using state-wise mobility data\n",
      "fips  27173 not found - using state-wise mobility data\n",
      "fips  28005 not found - using state-wise mobility data\n",
      "fips  28053 not found - using state-wise mobility data\n",
      "fips  28063 not found - using state-wise mobility data\n",
      "fips  28125 not found - using state-wise mobility data\n",
      "fips  29035 not found - using state-wise mobility data\n",
      "fips  29063 not found - using state-wise mobility data\n",
      "fips  29093 not found - using state-wise mobility data\n",
      "fips  29179 not found - using state-wise mobility data\n",
      "fips  29197 not found - using state-wise mobility data\n",
      "fips  29199 not found - using state-wise mobility data\n",
      "fips  29211 not found - using state-wise mobility data\n",
      "fips  29227 not found - using state-wise mobility data\n",
      "fips  30007 not found - using state-wise mobility data\n",
      "fips  30009 not found - using state-wise mobility data\n",
      "fips  30037 not found - using state-wise mobility data\n",
      "fips  30051 not found - using state-wise mobility data\n",
      "fips  30057 not found - using state-wise mobility data\n",
      "fips  30059 not found - using state-wise mobility data\n",
      "fips  30065 not found - using state-wise mobility data\n",
      "fips  30073 not found - using state-wise mobility data\n",
      "fips  30085 not found - using state-wise mobility data\n",
      "fips  30101 not found - using state-wise mobility data\n",
      "fips  30107 not found - using state-wise mobility data\n",
      "fips  31003 not found - using state-wise mobility data\n",
      "fips  31011 not found - using state-wise mobility data\n",
      "fips  31027 not found - using state-wise mobility data\n",
      "fips  31051 not found - using state-wise mobility data\n",
      "fips  31059 not found - using state-wise mobility data\n",
      "fips  31061 not found - using state-wise mobility data\n",
      "fips  31063 not found - using state-wise mobility data\n",
      "fips  31065 not found - using state-wise mobility data\n",
      "fips  31073 not found - using state-wise mobility data\n",
      "fips  31077 not found - using state-wise mobility data\n",
      "fips  31083 not found - using state-wise mobility data\n",
      "fips  31087 not found - using state-wise mobility data\n",
      "fips  31097 not found - using state-wise mobility data\n",
      "fips  31103 not found - using state-wise mobility data\n",
      "fips  31105 not found - using state-wise mobility data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fips  31107 not found - using state-wise mobility data\n",
      "fips  31123 not found - using state-wise mobility data\n",
      "fips  31125 not found - using state-wise mobility data\n",
      "fips  31129 not found - using state-wise mobility data\n",
      "fips  31143 not found - using state-wise mobility data\n",
      "fips  31161 not found - using state-wise mobility data\n",
      "fips  31163 not found - using state-wise mobility data\n",
      "fips  31167 not found - using state-wise mobility data\n",
      "fips  31171 not found - using state-wise mobility data\n",
      "fips  31175 not found - using state-wise mobility data\n",
      "fips  31181 not found - using state-wise mobility data\n",
      "fips  32015 not found - using state-wise mobility data\n",
      "fips  32017 not found - using state-wise mobility data\n",
      "fips  32021 not found - using state-wise mobility data\n",
      "fips  35003 not found - using state-wise mobility data\n",
      "fips  35021 not found - using state-wise mobility data\n",
      "fips  35059 not found - using state-wise mobility data\n",
      "fips  36041 not found - using state-wise mobility data\n",
      "fips  37075 not found - using state-wise mobility data\n",
      "fips  37095 not found - using state-wise mobility data\n",
      "fips  37177 not found - using state-wise mobility data\n",
      "fips  38005 not found - using state-wise mobility data\n",
      "fips  38009 not found - using state-wise mobility data\n",
      "fips  38011 not found - using state-wise mobility data\n",
      "fips  38013 not found - using state-wise mobility data\n",
      "fips  38023 not found - using state-wise mobility data\n",
      "fips  38025 not found - using state-wise mobility data\n",
      "fips  38027 not found - using state-wise mobility data\n",
      "fips  38029 not found - using state-wise mobility data\n",
      "fips  38031 not found - using state-wise mobility data\n",
      "fips  38037 not found - using state-wise mobility data\n",
      "fips  38049 not found - using state-wise mobility data\n",
      "fips  38051 not found - using state-wise mobility data\n",
      "fips  38055 not found - using state-wise mobility data\n",
      "fips  38063 not found - using state-wise mobility data\n",
      "fips  38065 not found - using state-wise mobility data\n",
      "fips  38067 not found - using state-wise mobility data\n",
      "fips  38069 not found - using state-wise mobility data\n",
      "fips  38073 not found - using state-wise mobility data\n",
      "fips  38075 not found - using state-wise mobility data\n",
      "fips  38079 not found - using state-wise mobility data\n",
      "fips  38081 not found - using state-wise mobility data\n",
      "fips  38085 not found - using state-wise mobility data\n",
      "fips  38087 not found - using state-wise mobility data\n",
      "fips  38091 not found - using state-wise mobility data\n",
      "fips  38103 not found - using state-wise mobility data\n",
      "fips  40007 not found - using state-wise mobility data\n",
      "fips  40025 not found - using state-wise mobility data\n",
      "fips  40053 not found - using state-wise mobility data\n",
      "fips  40059 not found - using state-wise mobility data\n",
      "fips  40067 not found - using state-wise mobility data\n",
      "fips  41023 not found - using state-wise mobility data\n",
      "fips  41025 not found - using state-wise mobility data\n",
      "fips  41055 not found - using state-wise mobility data\n",
      "fips  41063 not found - using state-wise mobility data\n",
      "fips  42023 not found - using state-wise mobility data\n",
      "fips  42053 not found - using state-wise mobility data\n",
      "fips  42113 not found - using state-wise mobility data\n",
      "fips  46003 not found - using state-wise mobility data\n",
      "fips  46009 not found - using state-wise mobility data\n",
      "fips  46017 not found - using state-wise mobility data\n",
      "fips  46023 not found - using state-wise mobility data\n",
      "fips  46025 not found - using state-wise mobility data\n",
      "fips  46031 not found - using state-wise mobility data\n",
      "fips  46037 not found - using state-wise mobility data\n",
      "fips  46039 not found - using state-wise mobility data\n",
      "fips  46043 not found - using state-wise mobility data\n",
      "fips  46047 not found - using state-wise mobility data\n",
      "fips  46049 not found - using state-wise mobility data\n",
      "fips  46057 not found - using state-wise mobility data\n",
      "fips  46059 not found - using state-wise mobility data\n",
      "fips  46067 not found - using state-wise mobility data\n",
      "fips  46069 not found - using state-wise mobility data\n",
      "fips  46073 not found - using state-wise mobility data\n",
      "fips  46085 not found - using state-wise mobility data\n",
      "fips  46087 not found - using state-wise mobility data\n",
      "fips  46089 not found - using state-wise mobility data\n",
      "fips  46091 not found - using state-wise mobility data\n",
      "fips  46097 not found - using state-wise mobility data\n",
      "fips  46101 not found - using state-wise mobility data\n",
      "fips  46102 not found - using state-wise mobility data\n",
      "fips  46109 not found - using state-wise mobility data\n",
      "fips  46111 not found - using state-wise mobility data\n",
      "fips  46115 not found - using state-wise mobility data\n",
      "fips  46117 not found - using state-wise mobility data\n",
      "fips  46119 not found - using state-wise mobility data\n",
      "fips  46121 not found - using state-wise mobility data\n",
      "fips  46123 not found - using state-wise mobility data\n",
      "fips  46125 not found - using state-wise mobility data\n",
      "fips  46129 not found - using state-wise mobility data\n",
      "fips  46137 not found - using state-wise mobility data\n",
      "fips  47067 not found - using state-wise mobility data\n",
      "fips  47137 not found - using state-wise mobility data\n",
      "fips  48011 not found - using state-wise mobility data\n",
      "fips  48045 not found - using state-wise mobility data\n",
      "fips  48079 not found - using state-wise mobility data\n",
      "fips  48081 not found - using state-wise mobility data\n",
      "fips  48087 not found - using state-wise mobility data\n",
      "fips  48095 not found - using state-wise mobility data\n",
      "fips  48101 not found - using state-wise mobility data\n",
      "fips  48107 not found - using state-wise mobility data\n",
      "fips  48125 not found - using state-wise mobility data\n",
      "fips  48153 not found - using state-wise mobility data\n",
      "fips  48169 not found - using state-wise mobility data\n",
      "fips  48173 not found - using state-wise mobility data\n",
      "fips  48191 not found - using state-wise mobility data\n",
      "fips  48205 not found - using state-wise mobility data\n",
      "fips  48207 not found - using state-wise mobility data\n",
      "fips  48275 not found - using state-wise mobility data\n",
      "fips  48295 not found - using state-wise mobility data\n",
      "fips  48305 not found - using state-wise mobility data\n",
      "fips  48319 not found - using state-wise mobility data\n",
      "fips  48345 not found - using state-wise mobility data\n",
      "fips  48359 not found - using state-wise mobility data\n",
      "fips  48393 not found - using state-wise mobility data\n",
      "fips  48417 not found - using state-wise mobility data\n",
      "fips  48421 not found - using state-wise mobility data\n",
      "fips  49017 not found - using state-wise mobility data\n",
      "fips  49031 not found - using state-wise mobility data\n",
      "fips  49037 not found - using state-wise mobility data\n",
      "fips  50009 not found - using state-wise mobility data\n",
      "fips  50013 not found - using state-wise mobility data\n",
      "fips  51045 not found - using state-wise mobility data\n",
      "fips  51049 not found - using state-wise mobility data\n",
      "fips  51077 not found - using state-wise mobility data\n",
      "fips  51081 not found - using state-wise mobility data\n",
      "fips  51091 not found - using state-wise mobility data\n",
      "fips  51097 not found - using state-wise mobility data\n",
      "fips  51115 not found - using state-wise mobility data\n",
      "fips  51181 not found - using state-wise mobility data\n",
      "fips  53013 not found - using state-wise mobility data\n",
      "fips  53019 not found - using state-wise mobility data\n",
      "fips  53043 not found - using state-wise mobility data\n",
      "fips  53059 not found - using state-wise mobility data\n",
      "fips  53069 not found - using state-wise mobility data\n",
      "fips  54013 not found - using state-wise mobility data\n",
      "fips  54015 not found - using state-wise mobility data\n",
      "fips  54047 not found - using state-wise mobility data\n",
      "fips  54063 not found - using state-wise mobility data\n",
      "fips  54071 not found - using state-wise mobility data\n",
      "fips  54073 not found - using state-wise mobility data\n",
      "fips  54093 not found - using state-wise mobility data\n",
      "fips  54105 not found - using state-wise mobility data\n",
      "fips  55007 not found - using state-wise mobility data\n",
      "fips  55037 not found - using state-wise mobility data\n",
      "fips  55041 not found - using state-wise mobility data\n",
      "fips  55051 not found - using state-wise mobility data\n",
      "fips  55091 not found - using state-wise mobility data\n",
      "fips  56011 not found - using state-wise mobility data\n",
      "fips  56017 not found - using state-wise mobility data\n",
      "fips  56027 not found - using state-wise mobility data\n",
      "fips  56043 not found - using state-wise mobility data\n"
     ]
    }
   ],
   "source": [
    "# retrieve mobility data. The data has been collected from from March 1st\n",
    "df_mobility = pd.read_csv('../data/us/mobility/DL-us-mobility-daterow.csv')\n",
    "df_mobility = df_mobility[~np.isnan(df_mobility.fips)]\n",
    "df_mobility[\"fips\"]=df_mobility[\"fips\"].apply(correct_FIPS)\n",
    "df_mobility = fix_FIPS(df_mobility, fipslabel=\"fips\", datelabel = \"date\", reduced=True)\n",
    "df_mobility.set_index(['date', 'fips'], inplace=True)\n",
    "\n",
    "df_mobility_eachCounty = []\n",
    "for fips in fipsList:\n",
    "    try: \n",
    "        df_mobility_eachCounty.append( df_mobility.xs(fips, level = 'fips') )\n",
    "    except  KeyError:\n",
    "        print('fips ', fips, 'not found - using state-wise mobility data')\n",
    "        state = df_fips2state.loc[fips, 'state']\n",
    "        if fips == '11001':\n",
    "            state = 'Washington, D.C.' # originally it's District of columnbia, which is not on mobility data\n",
    "            \n",
    "        df_mobility_eachCounty.append( df_mobility_orig[ (df_mobility_orig['admin_level']==1) & (df_mobility_orig['admin1']==state)] )\n",
    "        df_mobility_eachCounty[-1].set_index('date', inplace=True)\n",
    "        if len(df_mobility_eachCounty[-1])==0:\n",
    "            print('     caution: no data for ', fips)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seasonality = pd.read_csv('../exploratory_HJo/seasonality_stateLevel.csv', index_col='state')\n",
    "#print(df_seasonality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID Test Result data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testResult = pd.read_csv('../data/us/covid/daily_state_tests.csv')\n",
    "df_testResult.set_index(['date', 'fips'], inplace=True)\n",
    "\n",
    "df_state2fips = df_mobility_orig[ (df_mobility_orig['admin_level']==1)].copy()\n",
    "df_state2fips = df_state2fips.loc[:, ['admin1', 'fips']]\n",
    "df_state2fips.drop_duplicates(inplace = True)\n",
    "df_state2fips.set_index('admin1', inplace = True)\n",
    "\n",
    "df_testResult_eachCounty = []\n",
    "for fips in fipsList:\n",
    "    state = df_fips2state.loc[fips, 'state']\n",
    "    if fips == '11001':\n",
    "        state = 'Washington, D.C.' # originally it's District of columnbia, which is not on mobility data\n",
    "    state_fips = int(df_state2fips.loc[state, 'fips'])\n",
    "\n",
    "    df_testResult_eachCounty.append( df_testResult.xs(state_fips, level = 'fips') )\n",
    "    #df_testResult_eachCounty[-1].sort_index(axis = 0, ascending=True, inplace=True) \n",
    "    df_testResult_eachCounty[-1] = df_testResult_eachCounty[-1].sort_index(axis = 0, ascending=True) \n",
    "    df_testResult_eachCounty[-1] = df_testResult_eachCounty[-1].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function for concatenating dataframes - static values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatDF_static(data1, column1, fipsList, df_death_accum, minDeathNumber = 0, dic_cluster = []):\n",
    "    # inputs\n",
    "    #   data:               list of input dataframes\n",
    "    #   columnName:         list of lists storing column names of input dataframes to be used\n",
    "    #   fipsList:           list of fips of the counties to be used\n",
    "    #   minDeathNumber:     minimum number of accumulative death on the last day that the dataset should satisfy - counties with less number of death will be removed\n",
    "    \n",
    "    # divide the fipslist based on the minDeathNumber\n",
    "    totalNumDeath = [df_death_accum[x].iloc[-1, -1] for x in range(len(df_death_accum))]\n",
    "    fipsList = [fipsList[i] for i in range(len(fipsList)) if totalNumDeath[i]>=minDeathNumber]\n",
    "    \n",
    "    # save the original data\n",
    "    data1_orig = data1.copy()\n",
    "    \n",
    "    # go over each FIPS value\n",
    "    dataList = []\n",
    "    fips_noData = []\n",
    "    fips_final = []\n",
    "    for i, fips in enumerate(fipsList):\n",
    "        # for data1 (demographic)\n",
    "        try: \n",
    "            data1 = data1_orig.loc[fips, column1].to_numpy()\n",
    "            if len(dic_cluster)!=0:\n",
    "                data1 = np.append(data1, dic_cluster[fips])\n",
    "        except  KeyError:\n",
    "            print('fips ', fips, 'not found')\n",
    "            fips_noData.append(fips)\n",
    "            continue\n",
    "        fips_final.append(fips)\n",
    "        \n",
    "        dataList.append(data1)\n",
    "        \n",
    "    return dataList, fips_noData, fips_final\n",
    "\n",
    "\n",
    "# columns_demo = ['PopulationDensityperSqMile2010', 'PopTotalMale2017', 'PopTotalFemale2017',\n",
    "#                 'MedianAge2010', 'MedianAge,Male2010', 'MedianAge,Female2010',\n",
    "#                 '#Hospitals', '#ICU_beds', 'DiabetesPercentage', 'HeartDiseaseMortality',\n",
    "#                 'StrokeMortality', 'Smokers_Percentage', 'MedicareEnrollment,AgedTot2017']\n",
    "\n",
    "columns_demo = ['PopulationEstimate2018', 'Population(Persons)2017',\n",
    "       'PopTotalMale2017', 'PopTotalFemale2017', 'FracMale2017',\n",
    "       'PopulationEstimate65+2017', 'PopulationDensityperSqMile2010',\n",
    "       'CensusPopulation2010', 'MedianAge2010', 'MedianAge,Male2010',\n",
    "       'MedianAge,Female2010', '#EligibleforMedicare2018',\n",
    "       'MedicareEnrollment,AgedTot2017', '3-YrDiabetes2015-17',\n",
    "       'DiabetesPercentage', 'HeartDiseaseMortality', 'StrokeMortality',\n",
    "       'Smokers_Percentage', '#FTEHospitalTotal2017','#HospParticipatinginNetwork2017',\n",
    "       'TotalM.D. s,TotNon-FedandFed2017',\n",
    "       '#Hospitals', '#ICU_beds', 'dem_to_rep_ratio', 'PopMale<52010',\n",
    "       'PopFmle<52010', 'PopMale5-92010', 'PopFmle5-92010', 'PopMale10-142010',\n",
    "       'PopFmle10-142010', 'PopMale15-192010', 'PopFmle15-192010',\n",
    "       'PopMale20-242010', 'PopFmle20-242010', 'PopMale25-292010',\n",
    "       'PopFmle25-292010', 'PopMale30-342010', 'PopFmle30-342010',\n",
    "       'PopMale35-442010', 'PopFmle35-442010', 'PopMale45-542010',\n",
    "       'PopFmle45-542010', 'PopMale55-592010', 'PopFmle55-592010',\n",
    "       'PopMale60-642010', 'PopFmle60-642010', 'PopMale65-742010',\n",
    "       'PopFmle65-742010', 'PopMale75-842010', 'PopFmle75-842010',\n",
    "       'PopMale>842010', 'PopFmle>842010', '3-YrMortalityAge<1Year2015-17',\n",
    "       '3-YrMortalityAge1-4Years2015-17', '3-YrMortalityAge5-14Years2015-17',\n",
    "       '3-YrMortalityAge15-24Years2015-17',\n",
    "       '3-YrMortalityAge25-34Years2015-17',\n",
    "       '3-YrMortalityAge35-44Years2015-17',\n",
    "       '3-YrMortalityAge45-54Years2015-17',\n",
    "       '3-YrMortalityAge55-64Years2015-17',\n",
    "       '3-YrMortalityAge65-74Years2015-17',\n",
    "       '3-YrMortalityAge75-84Years2015-17', '3-YrMortalityAge85+Years2015-17',\n",
    "       'mortality2015-17Estimated', 'pclon10', 'pclat10', 'GDP_2015', 'GDP_2016', 'GDP_2017', 'GDP_2018']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for concatenating dataframes - timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from scipy import interpolate\n",
    "\n",
    "def concatDF_timeseries(data2, data3, data4, data5, column2, column3, column4, column5, fipsList, df_fips2state, data_demo, fipsList_demo, date_st, date_ed, \n",
    "             df_death_accum, minDeathNumber = 0, smoothData = False, exp = 2, removeNeg = True, normalizeTarget = False):\n",
    "    # inputs\n",
    "    #   data:               list of input dataframes\n",
    "    #   columnName:         list of lists storing column names of input dataframes to be used\n",
    "    #   fipsList:           list of fips of the counties to be used\n",
    "    #   fipsList_demo:      list of fips of the counties that were found in demogrpahic dataset\n",
    "    #   date_st:            starting date of the dataset to be used, in datetime variable\n",
    "    #   date_ed:            ending date of the dataset to be used, in datetime variable\n",
    "    #   minDeathNumber:     minimum number of accumulative death on the last day that the dataset should satisfy - counties with less number of death will be removed\n",
    "    #   smoothData:         whether to smooth the data with exponential filter\n",
    "    #   exp:                base of the exponential filter\n",
    "    #   removeNeg:          if it's true, change negative values into zeros when deaths<0\n",
    "    #   normalizeTarget:    if it's true, normalize the target (deaths) too\n",
    "    \n",
    "    # divide the fipslist based on the minDeathNumber\n",
    "    totalNumDeath = [df_death_accum[x].iloc[-1, -1] for x in range(len(df_death_accum))]\n",
    "    fipsList = [fipsList[i] for i in range(len(fipsList)) if totalNumDeath[i]>=minDeathNumber]\n",
    "    data2 = [data2[i] for i in range(len(fipsList)) if fipsList[i] in fipsList]\n",
    "\n",
    "    # handle dates\n",
    "    date_st_str = date_st.strftime(\"%Y-%m-%d\")\n",
    "    date_ed_str = date_ed.strftime(\"%Y-%m-%d\")\n",
    "    date_st_str_test = date_st.strftime(\"%Y%m%d\")  # for test result dataframe\n",
    "    date_ed_str_test = date_ed.strftime(\"%Y%m%d\")  # for test result dataframe\n",
    "    numDays = (date_ed-date_st).days+1\n",
    "    date_st_seasonality = date_st - relativedelta(years=3)  # seasonality data has been saved based on 2017 data\n",
    "    date_st_seasonality = date_st_seasonality.strftime(\"%Y-%m-%d\")\n",
    "    date_ed_seasonality = date_ed - relativedelta(years=3)\n",
    "    date_ed_seasonality = date_ed_seasonality.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # save the original data\n",
    "    data2_orig = data2.copy()\n",
    "    data3_orig = data3.copy()\n",
    "    data4_orig = data4.copy()\n",
    "    data5_orig = data5.copy()\n",
    "    \n",
    "    # go over each FIPS value\n",
    "    dataList = []\n",
    "    fips_noData = []\n",
    "    fips_final = []\n",
    "    for i, fips in enumerate(fipsList):\n",
    "        if fips not in fipsList_demo:\n",
    "            continue\n",
    "        \n",
    "        # for data2 (mortality)\n",
    "        data2 = data2_orig[i].loc[date_st_str:date_ed_str, column2].to_numpy()\n",
    "        if data2.shape[0]<numDays:\n",
    "            data2 = np.concatenate( (np.zeros((numDays-data2.shape[0], len(column2))), data2), axis = 0)        \n",
    "        if removeNeg: \n",
    "            data2[data2[:,1]<0, 1] = 0\n",
    "        if smoothData:\n",
    "            for i in range(data2.shape[0]):\n",
    "                if i == 0: continue\n",
    "                data2[i, 1] = data2[i, 1]+data2[i-1, 1]/exp\n",
    "        \n",
    "        # for data3 (mobility)\n",
    "        data3 = data3_orig[i].loc[date_st_str:date_ed_str, column3].to_numpy()\n",
    "        # If there are days without mobility data, copy and paste the first & last mobility data for the dates before & after the existing data\n",
    "        # For the rest of the data set, use spline interpolation\n",
    "        if len(data3)<numDays and len(column3)!=0:\n",
    "            data3_temp = data3_orig[i].loc[date_st_str:date_ed_str, column3]\n",
    "            \n",
    "            # get dates on dataframe\n",
    "            date = [datetime.datetime.strptime(x, \"%Y-%m-%d\") for x in data3_temp.index]  # given dates shown on df\n",
    "            \n",
    "            # figure out the first and last date on the dataset\n",
    "            date_ed_temp = date[-1]  # last date in the data\n",
    "            date_st_temp = date[0]   # first date in the data\n",
    "            numDays_temp = (date_ed_temp - date_st_temp).days+1\n",
    "            numDays_bef = (date_st_temp - datetime.datetime.combine(date_st, datetime.time(0, 0))).days  # number of dates with missing data in the beginning\n",
    "            numDays_aft = (datetime.datetime.combine(date_ed, datetime.time(0, 0)) - date_ed_temp).days  # number of dates with missing data at the end\n",
    "            \n",
    "            # convert dates into timestamps. timestamp is for converting dates into numpy array for spline interpolation\n",
    "            date_timestamp = np.array([calendar.timegm(x.timetuple()) for x in date])\n",
    "            date_mid = [date_st_temp + datetime.timedelta(days=x) for x in range(numDays_temp)]  # full list of dates from the first to the last date on a dataset\n",
    "            date_mid_timestamp = np.array([calendar.timegm(x.timetuple()) for x in date_mid])\n",
    "            \n",
    "            data3_mid = []\n",
    "            for x, column3_each in enumerate(column3):  # go over each column specified in column3\n",
    "                spl = interpolate.splrep(date_timestamp, data3[:, x], s=0)\n",
    "                data3_mid.append( interpolate.splev(date_mid_timestamp, spl, der=0) )\n",
    "            data3 = np.transpose(np.array(data3_mid).copy())\n",
    "            \n",
    "            # fill the beginning and the end of the data by copying & pasting the first & last data\n",
    "            if numDays_bef!=0:\n",
    "                data3_bef = np.kron( np.ones((numDays_bef,1)), data3[0, :])\n",
    "                data3 = np.vstack( (data3_bef, data3) )\n",
    "            if numDays_aft!=0:\n",
    "                data3_aft = np.kron( np.ones((numDays_aft,1)), data3[-1, :])\n",
    "                data3 = np.vstack( (data3, data3_aft) )\n",
    "                    \n",
    "        # for data4 (seasonality)\n",
    "        state = df_fips2state.loc[fips, 'state']\n",
    "        data4 = data4_orig.loc[state, :]\n",
    "        data4.set_index('date', inplace=True)\n",
    "        data4 = data4.loc[date_st_seasonality:date_ed_seasonality, column4]\n",
    "        \n",
    "        # for data5 (test result data)\n",
    "        if len(column5)!=0:\n",
    "            data5 = data5_orig[i].loc[date_st_str_test:date_ed_str_test, column5].to_numpy()\n",
    "            # If there are days without test result data, copy and paste the first & last data for the dates before & after the existing data\n",
    "            # For the rest of the data set, use spline interpolation\n",
    "            if len(data5)<numDays:\n",
    "                data5_temp = data5_orig[i].loc[date_st_str_test:date_ed_str_test, column5]\n",
    "\n",
    "                # get dates on dataframe\n",
    "                date = [datetime.datetime.strptime(str(x), \"%Y%m%d\") for x in data5_temp.index]  # given dates shown on df\n",
    "\n",
    "                # figure out the first and last date on the dataset\n",
    "                date_ed_temp = date[-1]  # last date in the data\n",
    "                date_st_temp = date[0]   # first date in the data\n",
    "                numDays_temp = (date_ed_temp - date_st_temp).days+1\n",
    "                numDays_bef = (date_st_temp - datetime.datetime.combine(date_st, datetime.time(0, 0))).days  # number of dates with missing data in the beginning\n",
    "                numDays_aft = (datetime.datetime.combine(date_ed, datetime.time(0, 0)) - date_ed_temp).days  # number of dates with missing data at the end\n",
    "\n",
    "                # convert dates into timestamps. timestamp is for converting dates into numpy array for spline interpolation\n",
    "                date_timestamp = np.array([calendar.timegm(x.timetuple()) for x in date])\n",
    "                date_mid = [date_st_temp + datetime.timedelta(days=x) for x in range(numDays_temp)]  # full list of dates from the first to the last date on a dataset\n",
    "                date_mid_timestamp = np.array([calendar.timegm(x.timetuple()) for x in date_mid])\n",
    "\n",
    "                data5_mid = []\n",
    "                for x, column5_each in enumerate(column5):  # go over each column specified in column3\n",
    "                    spl = interpolate.splrep(date_timestamp, data5[:, x], s=0)\n",
    "                    data5_mid.append( interpolate.splev(date_mid_timestamp, spl, der=0) )\n",
    "                data5 = np.transpose(np.array(data5_mid).copy())\n",
    "\n",
    "                # fill the beginning and the end of the data by copying & pasting the first & last data\n",
    "                if numDays_bef!=0:\n",
    "                    data5_bef = np.kron( np.ones((numDays_bef,1)), data5[0, :])\n",
    "                    data5 = np.vstack( (data5_bef, data5) )\n",
    "                if numDays_aft!=0:\n",
    "                    data5_aft = np.kron( np.ones((numDays_aft,1)), data5[-1, :])\n",
    "                    data5 = np.vstack( (data5, data5_aft) )\n",
    "                \n",
    "        if len(column3)!=0:\n",
    "            data = np.hstack( (data2, data3) )    \n",
    "        else:\n",
    "            data = data2\n",
    "            \n",
    "        if len(column4)!=0:\n",
    "            data = np.hstack( (data, data4) )    \n",
    "        \n",
    "        if len(column5)!=0:\n",
    "            data = np.hstack( (data, data5) )    \n",
    "        \n",
    "        dataList.append(data)\n",
    "        \n",
    "    return dataList, fips_noData, fips_final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "date_st = datetime.date(2020, 3, 1)\n",
    "date_ed = datetime.date(2020, 5, 21)\n",
    "normalizeTarget = False\n",
    "\n",
    "columns_mortality = ['cases', 'deaths', 'weekday']\n",
    "columns_mobility = ['m50', 'm50_index']\n",
    "columns_season = ['seasonality']\n",
    "# columns_testResult = ['positive', 'negative', 'hospitalizedCurrently', 'inIcuCurrently', 'onVentilatorCurrently']\n",
    "columns_testResult = ['positive', 'negative', 'pending', 'hospitalizedCurrently',\n",
    "       'hospitalizedCumulative', 'inIcuCurrently', 'inIcuCumulative',\n",
    "       'onVentilatorCurrently', 'onVentilatorCumulative', 'recovered', \n",
    "       'hospitalized', 'total', 'totalTestResults', 'posNeg', 'deathIncrease',\n",
    "       'hospitalizedIncrease', 'negativeIncrease', 'positiveIncrease',\n",
    "       'totalTestResultsIncrease'] # removed dataQualityGrade, lastUpdateEt, hash, dateChecked, as they are not numerical\n",
    "    # also removed death as it's redundant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_berk_withPCA = df_berk2.copy()\n",
    "columns_lowCorr = ['pclon10', 'HeartDiseaseMortality', 'DiabetesPercentage','MedianAge2010', 'MedianAge,Male2010','MedianAge,Female2010',\n",
    "                   'StrokeMortality', 'Smokers_Percentage', 'pclat10','FracMale2017', 'dem_to_rep_ratio', 'PopulationDensityperSqMile2010']\n",
    "columns_highCorr = ['mortality2015-17Estimated', 'PopTotalFemale2017',\n",
    "       '3-YrDiabetes2015-17', \n",
    "       'CensusPopulation2010', \n",
    "       '#ICU_beds', 'PopulationEstimate65+2017',\n",
    "       '#FTEHospitalTotal2017', '3-YrMortalityAge<1Year2015-17',\n",
    "       '3-YrMortalityAge1-4Years2015-17', '#Hospitals',\n",
    "       'PopFmle>842010', 'TotalM.D. s,TotNon-FedandFed2017', 'GDP_2017',\n",
    "       '#HospParticipatinginNetwork2017', 'GDP_2016', 'GDP_2015', \n",
    "       'PopFmle75-842010', 'PopMale>842010', '3-YrMortalityAge85+Years2015-17',\n",
    "       '3-YrMortalityAge75-84Years2015-17',\n",
    "       '3-YrMortalityAge15-24Years2015-17', 'PopFmle65-742010',\n",
    "       'PopMale75-842010', '3-YrMortalityAge5-14Years2015-17',\n",
    "       'PopFmle15-192010', '3-YrMortalityAge25-34Years2015-17',\n",
    "       'PopMale20-242010', 'PopFmle20-242010',\n",
    "       'MedicareEnrollment,AgedTot2017', 'PopMale25-292010',\n",
    "       'PopFmle30-342010', '3-YrMortalityAge65-74Years2015-17',\n",
    "       'PopFmle25-292010', '3-YrMortalityAge55-64Years2015-17',\n",
    "       'PopFmle<52010', 'PopMale30-342010', 'PopMale<52010',\n",
    "       '3-YrMortalityAge45-54Years2015-17', 'PopMale5-92010',\n",
    "       'PopMale35-442010',\n",
    "       '3-YrMortalityAge35-44Years2015-17', 'PopFmle5-92010',\n",
    "       'PopMale10-142010', 'PopFmle10-142010', 'PopMale15-192010',\n",
    "       'PopFmle60-642010', '#EligibleforMedicare2018',\n",
    "       'PopMale65-742010', 'Population(Persons)2017', \n",
    "       'PopulationEstimate2018', 'PopFmle35-442010',\n",
    "       'PopTotalMale2017', \n",
    "       'PopMale45-542010', 'PopFmle55-592010', 'PopMale60-642010',\n",
    "       'PopFmle45-542010', 'PopMale55-592010']\n",
    "\n",
    "df_berk_befPCA = df_berk_withPCA[columns_highCorr]\n",
    "df_berk_withPCA = df_berk_withPCA[columns_lowCorr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# df_berk_befPCA_np = df_berk_befPCA.to_numpy()\n",
    "# df_berk_befPCA_np.shape\n",
    "df_berk_befPCA = StandardScaler().fit_transform(df_berk_befPCA)\n",
    "pca = PCA(.999)\n",
    "pc = pca.fit_transform(df_berk_befPCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3113, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclon10</th>\n",
       "      <th>HeartDiseaseMortality</th>\n",
       "      <th>DiabetesPercentage</th>\n",
       "      <th>MedianAge2010</th>\n",
       "      <th>MedianAge,Male2010</th>\n",
       "      <th>MedianAge,Female2010</th>\n",
       "      <th>StrokeMortality</th>\n",
       "      <th>Smokers_Percentage</th>\n",
       "      <th>pclat10</th>\n",
       "      <th>FracMale2017</th>\n",
       "      <th>...</th>\n",
       "      <th>pc8</th>\n",
       "      <th>pc9</th>\n",
       "      <th>pc10</th>\n",
       "      <th>pc11</th>\n",
       "      <th>pc12</th>\n",
       "      <th>pc13</th>\n",
       "      <th>pc14</th>\n",
       "      <th>pc15</th>\n",
       "      <th>pc16</th>\n",
       "      <th>pc17</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>countyFIPS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>01001</td>\n",
       "      <td>-86.494165</td>\n",
       "      <td>204.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>37.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>37.9</td>\n",
       "      <td>56.1</td>\n",
       "      <td>18.081557</td>\n",
       "      <td>32.500389</td>\n",
       "      <td>0.486578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051440</td>\n",
       "      <td>0.046641</td>\n",
       "      <td>0.037642</td>\n",
       "      <td>0.130746</td>\n",
       "      <td>0.031407</td>\n",
       "      <td>0.090717</td>\n",
       "      <td>0.004226</td>\n",
       "      <td>-0.025012</td>\n",
       "      <td>0.065763</td>\n",
       "      <td>-0.054134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01003</td>\n",
       "      <td>-87.762381</td>\n",
       "      <td>183.2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>41.1</td>\n",
       "      <td>40.1</td>\n",
       "      <td>42.2</td>\n",
       "      <td>41.9</td>\n",
       "      <td>17.489033</td>\n",
       "      <td>30.548923</td>\n",
       "      <td>0.485472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242081</td>\n",
       "      <td>-0.149226</td>\n",
       "      <td>-0.163714</td>\n",
       "      <td>0.288925</td>\n",
       "      <td>0.169704</td>\n",
       "      <td>-0.336700</td>\n",
       "      <td>0.068208</td>\n",
       "      <td>-0.018095</td>\n",
       "      <td>-0.136203</td>\n",
       "      <td>0.121896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01005</td>\n",
       "      <td>-85.310038</td>\n",
       "      <td>220.4</td>\n",
       "      <td>15.7</td>\n",
       "      <td>39.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>41.6</td>\n",
       "      <td>49.0</td>\n",
       "      <td>21.999985</td>\n",
       "      <td>31.844036</td>\n",
       "      <td>0.527701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038018</td>\n",
       "      <td>-0.038875</td>\n",
       "      <td>-0.021113</td>\n",
       "      <td>0.014349</td>\n",
       "      <td>0.064904</td>\n",
       "      <td>-0.010696</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>-0.070605</td>\n",
       "      <td>-0.033789</td>\n",
       "      <td>0.049312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01007</td>\n",
       "      <td>-87.127659</td>\n",
       "      <td>225.5</td>\n",
       "      <td>13.3</td>\n",
       "      <td>37.8</td>\n",
       "      <td>36.5</td>\n",
       "      <td>39.5</td>\n",
       "      <td>57.2</td>\n",
       "      <td>19.114200</td>\n",
       "      <td>33.030921</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024051</td>\n",
       "      <td>-0.072956</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.032162</td>\n",
       "      <td>0.053402</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>-0.053591</td>\n",
       "      <td>-0.027796</td>\n",
       "      <td>0.044010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01009</td>\n",
       "      <td>-86.591491</td>\n",
       "      <td>224.8</td>\n",
       "      <td>14.9</td>\n",
       "      <td>39.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>52.8</td>\n",
       "      <td>19.208672</td>\n",
       "      <td>33.955243</td>\n",
       "      <td>0.493114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034623</td>\n",
       "      <td>0.010812</td>\n",
       "      <td>-0.045204</td>\n",
       "      <td>0.047260</td>\n",
       "      <td>0.136343</td>\n",
       "      <td>0.050106</td>\n",
       "      <td>0.166609</td>\n",
       "      <td>0.087799</td>\n",
       "      <td>0.090388</td>\n",
       "      <td>0.156895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              pclon10  HeartDiseaseMortality  DiabetesPercentage  \\\n",
       "countyFIPS                                                         \n",
       "01001      -86.494165                  204.5                 9.9   \n",
       "01003      -87.762381                  183.2                 8.5   \n",
       "01005      -85.310038                  220.4                15.7   \n",
       "01007      -87.127659                  225.5                13.3   \n",
       "01009      -86.591491                  224.8                14.9   \n",
       "\n",
       "            MedianAge2010  MedianAge,Male2010  MedianAge,Female2010  \\\n",
       "countyFIPS                                                            \n",
       "01001                37.0                35.9                  37.9   \n",
       "01003                41.1                40.1                  42.2   \n",
       "01005                39.0                37.2                  41.6   \n",
       "01007                37.8                36.5                  39.5   \n",
       "01009                39.0                38.0                  40.0   \n",
       "\n",
       "            StrokeMortality  Smokers_Percentage    pclat10  FracMale2017  ...  \\\n",
       "countyFIPS                                                                ...   \n",
       "01001                  56.1           18.081557  32.500389      0.486578  ...   \n",
       "01003                  41.9           17.489033  30.548923      0.485472  ...   \n",
       "01005                  49.0           21.999985  31.844036      0.527701  ...   \n",
       "01007                  57.2           19.114200  33.030921      0.535469  ...   \n",
       "01009                  52.8           19.208672  33.955243      0.493114  ...   \n",
       "\n",
       "                 pc8       pc9      pc10      pc11      pc12      pc13  \\\n",
       "countyFIPS                                                               \n",
       "01001      -0.051440  0.046641  0.037642  0.130746  0.031407  0.090717   \n",
       "01003       0.242081 -0.149226 -0.163714  0.288925  0.169704 -0.336700   \n",
       "01005       0.038018 -0.038875 -0.021113  0.014349  0.064904 -0.010696   \n",
       "01007       0.024051 -0.072956  0.004493  0.032162  0.053402  0.005343   \n",
       "01009      -0.034623  0.010812 -0.045204  0.047260  0.136343  0.050106   \n",
       "\n",
       "                pc14      pc15      pc16      pc17  \n",
       "countyFIPS                                          \n",
       "01001       0.004226 -0.025012  0.065763 -0.054134  \n",
       "01003       0.068208 -0.018095 -0.136203  0.121896  \n",
       "01005       0.010003 -0.070605 -0.033789  0.049312  \n",
       "01007       0.000336 -0.053591 -0.027796  0.044010  \n",
       "01009       0.166609  0.087799  0.090388  0.156895  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pc.shape)\n",
    "test = pd.DataFrame(pc)\n",
    "test.index = df_berk_withPCA.index\n",
    "test.columns = ['pc%d' % (x) for x in range(0,18)]\n",
    "test.head()\n",
    "#pc.shape\n",
    "#dataList_static, fips_noData_static, fips_final_static = concatDF_static(df_berk2, columns_demo, fipsList, df_nyt_eachCounty_accum, minDeathNumber=0, dic_cluster=[])\n",
    "\n",
    "#df_berk_withPCA = pd.concat([df_berk_withPCA, pd.DataFrame(pc)], axis=1)\n",
    "#df_berk_withPCA.join(pd.DataFrame(pc))\n",
    "df_berk_withPCA = pd.concat([df_berk_withPCA, test], axis=1)\n",
    "df_berk_withPCA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pclon10', 'HeartDiseaseMortality', 'DiabetesPercentage', 'MedianAge2010', 'MedianAge,Male2010', 'MedianAge,Female2010', 'StrokeMortality', 'Smokers_Percentage', 'pclat10', 'FracMale2017', 'dem_to_rep_ratio', 'PopulationDensityperSqMile2010', 'pc0', 'pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8', 'pc9', 'pc10', 'pc11', 'pc12', 'pc13', 'pc14', 'pc15', 'pc16', 'pc17']\n"
     ]
    }
   ],
   "source": [
    "pcaList = ['pc%d' % (x) for x in range(0,18)]\n",
    "columns_aftPCA = columns_lowCorr + pcaList\n",
    "print(columns_aftPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fips  02050 not found\n",
      "fips  02090 not found\n",
      "fips  02110 not found\n",
      "fips  02122 not found\n",
      "fips  02130 not found\n",
      "fips  02150 not found\n",
      "fips  02170 not found\n",
      "fips  02180 not found\n",
      "fips  02188 not found\n",
      "fips  02195 not found\n",
      "fips  02201 not found\n",
      "fips  02220 not found\n",
      "fips  02240 not found\n",
      "fips  02261 not found\n",
      "fips  02290 not found\n",
      "fips  08014 not found\n",
      "fips  46102 not found\n"
     ]
    }
   ],
   "source": [
    "columns_aftPCA_chosen = ['pc0']\n",
    "dataList_static_PCA, fips_noData_static_PCA, fips_final_static_PCA = concatDF_static(\n",
    "    df_berk_withPCA, columns_aftPCA_chosen, fipsList, df_nyt_eachCounty_accum, minDeathNumber=0, dic_cluster=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cases', 'deaths', 'weekday', 'weekday', 'positive', 'hospitalizedCumulative', 'recovered', 'hospitalized', 'deathIncrease', 'positiveIncrease', 'totalTestResultsIncrease']\n"
     ]
    }
   ],
   "source": [
    "columns_time_selected = ['cases', 'deaths', 'weekday', 'positive', 'hospitalizedCumulative', 'recovered', 'hospitalized', \n",
    "                         'deathIncrease', 'positiveIncrease', 'totalTestResultsIncrease']\n",
    "\n",
    "# choose features from time-series data\n",
    "columns_mortality = ['cases', 'deaths', 'weekday']\n",
    "columns_mobility = ['m50', 'm50_index']\n",
    "columns_season = ['seasonality']\n",
    "columns_testResult = ['positive', 'negative', 'pending', 'hospitalizedCurrently',\n",
    "       'hospitalizedCumulative', 'inIcuCurrently', 'inIcuCumulative',\n",
    "       'onVentilatorCurrently', 'onVentilatorCumulative', 'recovered', \n",
    "       'hospitalized', 'total', 'totalTestResults', 'posNeg', 'deathIncrease',\n",
    "       'hospitalizedIncrease', 'negativeIncrease', 'positiveIncrease',\n",
    "       'totalTestResultsIncrease'] \n",
    "columns_mortality_tmp = [ x for x in columns_mortality if x in columns_time_selected]\n",
    "columns_mortality_tmp = columns_mortality_tmp + ['weekday']\n",
    "columns_mobility_tmp = [ x for x in columns_mobility if x in columns_time_selected]\n",
    "columns_season_tmp = [ x for x in columns_season if x in columns_time_selected]\n",
    "columns_testResult_tmp = [ x for x in columns_testResult if x in columns_time_selected]\n",
    "columns_time_selected_reorder = columns_mortality_tmp+columns_mobility_tmp+columns_season_tmp+columns_testResult_tmp\n",
    "dataList_time, fips_noData_time, fips_final_time = concatDF_timeseries(df_nyt_eachCounty, df_mobility_eachCounty, df_seasonality, df_testResult_eachCounty,\n",
    "                                                         columns_mortality_tmp, columns_mobility_tmp, columns_season_tmp, columns_testResult_tmp,\n",
    "                                                         fipsList, df_fips2state, df_berk2, fips_final_static_PCA, date_st, date_ed, df_nyt_eachCounty_accum, 0)\n",
    "print(columns_time_selected_reorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataList_static_PCA[0].shape\n",
    "#len(dataList_static_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 136770 samples, validate on 2910 samples\n",
      "Epoch 1/25\n",
      "136770/136770 [==============================] - 24s 176us/sample - loss: 0.1240 - val_loss: 0.1573\n",
      "Epoch 2/25\n",
      "136770/136770 [==============================] - 23s 172us/sample - loss: 0.1122 - val_loss: 0.1679\n",
      "Epoch 3/25\n",
      "136770/136770 [==============================] - 24s 172us/sample - loss: 0.1083 - val_loss: 0.1448\n",
      "Epoch 4/25\n",
      "136770/136770 [==============================] - 23s 170us/sample - loss: 0.1054 - val_loss: 0.1488\n",
      "Epoch 5/25\n",
      "136770/136770 [==============================] - 23s 171us/sample - loss: 0.1035 - val_loss: 0.1438\n",
      "Epoch 6/25\n",
      "136770/136770 [==============================] - 24s 176us/sample - loss: 0.1017 - val_loss: 0.1500\n",
      "Epoch 7/25\n",
      "136770/136770 [==============================] - 23s 171us/sample - loss: 0.1004 - val_loss: 0.1486\n",
      "Epoch 8/25\n",
      "136770/136770 [==============================] - 23s 171us/sample - loss: 0.0994 - val_loss: 0.1481\n",
      "Epoch 9/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0982 - val_loss: 0.1495\n",
      "Epoch 10/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0970 - val_loss: 0.1512\n",
      "Epoch 11/25\n",
      "136770/136770 [==============================] - 24s 172us/sample - loss: 0.0959 - val_loss: 0.1453\n",
      "Epoch 12/25\n",
      "136770/136770 [==============================] - 23s 172us/sample - loss: 0.0948 - val_loss: 0.1525\n",
      "Epoch 13/25\n",
      "136770/136770 [==============================] - 24s 172us/sample - loss: 0.0940 - val_loss: 0.1531\n",
      "Epoch 14/25\n",
      "136770/136770 [==============================] - 23s 172us/sample - loss: 0.0931 - val_loss: 0.1502\n",
      "Epoch 15/25\n",
      "136770/136770 [==============================] - 23s 172us/sample - loss: 0.0923 - val_loss: 0.1473\n",
      "Epoch 16/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0916 - val_loss: 0.1525\n",
      "Epoch 17/25\n",
      "136770/136770 [==============================] - 24s 172us/sample - loss: 0.0910 - val_loss: 0.1485\n",
      "Epoch 18/25\n",
      "136770/136770 [==============================] - 24s 172us/sample - loss: 0.0902 - val_loss: 0.1478\n",
      "Epoch 19/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0896 - val_loss: 0.1516\n",
      "Epoch 20/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0893 - val_loss: 0.1494\n",
      "Epoch 21/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0886 - val_loss: 0.1521\n",
      "Epoch 22/25\n",
      "136770/136770 [==============================] - 24s 172us/sample - loss: 0.0882 - val_loss: 0.1517\n",
      "Epoch 23/25\n",
      "136770/136770 [==============================] - 23s 171us/sample - loss: 0.0875 - val_loss: 0.1508\n",
      "Epoch 24/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0870 - val_loss: 0.1519\n",
      "Epoch 25/25\n",
      "136770/136770 [==============================] - 24s 172us/sample - loss: 0.0864 - val_loss: 0.1526\n",
      "Train on 136770 samples, validate on 2910 samples\n",
      "Epoch 1/25\n",
      "136770/136770 [==============================] - 24s 178us/sample - loss: 0.1241 - val_loss: 0.1763\n",
      "Epoch 2/25\n",
      "136770/136770 [==============================] - 23s 172us/sample - loss: 0.1123 - val_loss: 0.1457\n",
      "Epoch 3/25\n",
      "136770/136770 [==============================] - 23s 171us/sample - loss: 0.1077 - val_loss: 0.1455\n",
      "Epoch 4/25\n",
      "136770/136770 [==============================] - 24s 176us/sample - loss: 0.1049 - val_loss: 0.1448\n",
      "Epoch 5/25\n",
      "136770/136770 [==============================] - 24s 177us/sample - loss: 0.1031 - val_loss: 0.1443\n",
      "Epoch 6/25\n",
      "136770/136770 [==============================] - 24s 172us/sample - loss: 0.1014 - val_loss: 0.1428\n",
      "Epoch 7/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.1002 - val_loss: 0.1457\n",
      "Epoch 8/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0992 - val_loss: 0.1415\n",
      "Epoch 9/25\n",
      "136770/136770 [==============================] - 24s 172us/sample - loss: 0.0978 - val_loss: 0.1492\n",
      "Epoch 10/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0967 - val_loss: 0.1429\n",
      "Epoch 11/25\n",
      "136770/136770 [==============================] - 24s 174us/sample - loss: 0.0955 - val_loss: 0.1461\n",
      "Epoch 12/25\n",
      "136770/136770 [==============================] - 24s 175us/sample - loss: 0.0948 - val_loss: 0.1423\n",
      "Epoch 13/25\n",
      "136770/136770 [==============================] - 24s 172us/sample - loss: 0.0938 - val_loss: 0.1439\n",
      "Epoch 14/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0930 - val_loss: 0.1421\n",
      "Epoch 15/25\n",
      "136770/136770 [==============================] - 24s 174us/sample - loss: 0.0924 - val_loss: 0.1487\n",
      "Epoch 16/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0918 - val_loss: 0.1433\n",
      "Epoch 17/25\n",
      "136770/136770 [==============================] - 24s 174us/sample - loss: 0.0909 - val_loss: 0.1432\n",
      "Epoch 18/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0906 - val_loss: 0.1473\n",
      "Epoch 19/25\n",
      "136770/136770 [==============================] - 24s 173us/sample - loss: 0.0900 - val_loss: 0.1464\n",
      "Epoch 20/25\n",
      "136770/136770 [==============================] - 24s 175us/sample - loss: 0.0892 - val_loss: 0.1450\n",
      "Epoch 21/25\n",
      "136770/136770 [==============================] - 24s 175us/sample - loss: 0.0886 - val_loss: 0.1468\n",
      "Epoch 22/25\n",
      "136770/136770 [==============================] - 24s 176us/sample - loss: 0.0882 - val_loss: 0.1477\n",
      "Epoch 23/25\n",
      "136770/136770 [==============================] - 24s 176us/sample - loss: 0.0877 - val_loss: 0.1538\n",
      "Epoch 24/25\n",
      "136770/136770 [==============================] - 24s 175us/sample - loss: 0.0871 - val_loss: 0.1494\n",
      "Epoch 25/25\n",
      "136770/136770 [==============================] - 24s 176us/sample - loss: 0.0865 - val_loss: 0.1485\n"
     ]
    }
   ],
   "source": [
    "inputSize = 7\n",
    "normalizeTarget = False\n",
    "\n",
    "quantile = 0.5\n",
    "num_epoch = 25\n",
    "\n",
    "NUM_CELLS = 256\n",
    "NUM_DAYS_OUT = 14\n",
    "\n",
    "num_repeat = 2\n",
    "modelList = []\n",
    "historyList = []\n",
    "\n",
    "\n",
    "# normalization - static\n",
    "data_static_zscore, data_static_mean, data_static_std = normalizeData(dataList_static_PCA)\n",
    "\n",
    "# normalization - time\n",
    "data_time_zscore, data_time_mean, data_time_std = normalizeData(dataList_time)\n",
    "\n",
    "# for static dataset\n",
    "trainingData_static, testingData_static = dataPrep_LSTM_fromNP_static(data_static_zscore, data_time_zscore, inputSize, 14, 1, 14)\n",
    "\n",
    "# for timeseries dataset\n",
    "trainingData_time, trainingAns_time, testingData_time, testingAns_time = dataPrep_LSTM_fromNP_time(data_time_zscore, 'deaths', inputSize, 14, 1, 14, columns_time_selected_reorder)\n",
    "\n",
    "# unnormalize the targets\n",
    "if not normalizeTarget:\n",
    "    trainingAns_time, testingAns_time = unnormalizeTarget(trainingAns_time, testingAns_time, data_time_mean, data_time_std, columns_time_selected_reorder, 'deaths')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# train the model\n",
    "for rep in range(num_repeat):\n",
    "    model = condLSTM(NUM_CELLS, NUM_DAYS_OUT)\n",
    "    model.call([trainingData_time, trainingData_static])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss=lambda y_p, y: quantileLoss(quantile, y_p, y))\n",
    "    history = model.fit(x=[trainingData_time, trainingData_static], y=trainingAns_time,\n",
    "              validation_data=([testingData_time, testingData_static], testingAns_time),\n",
    "              epochs=num_epoch, shuffle=True, batch_size=64)\n",
    "    modelList.append(model)\n",
    "    historyList.append(history)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 136770 samples, validate on 2910 samples\n",
      "Epoch 1/25\n",
      "136770/136770 [==============================] - 82s 600us/sample - loss: 0.1223 - val_loss: 0.1668\n",
      "Epoch 2/25\n",
      "136770/136770 [==============================] - 81s 595us/sample - loss: 0.1114 - val_loss: 0.1559\n",
      "Epoch 3/25\n",
      "136770/136770 [==============================] - 82s 597us/sample - loss: 0.1071 - val_loss: 0.1513\n",
      "Epoch 4/25\n",
      "136770/136770 [==============================] - 82s 599us/sample - loss: 0.1045 - val_loss: 0.1456\n",
      "Epoch 5/25\n",
      "136770/136770 [==============================] - 82s 603us/sample - loss: 0.1029 - val_loss: 0.1443\n",
      "Epoch 6/25\n",
      "136770/136770 [==============================] - 82s 601us/sample - loss: 0.1009 - val_loss: 0.1492\n",
      "Epoch 7/25\n",
      "136770/136770 [==============================] - 82s 599us/sample - loss: 0.0998 - val_loss: 0.1473\n",
      "Epoch 8/25\n",
      "136770/136770 [==============================] - 82s 599us/sample - loss: 0.0984 - val_loss: 0.1474\n",
      "Epoch 9/25\n",
      "136770/136770 [==============================] - 82s 599us/sample - loss: 0.0971 - val_loss: 0.1495\n",
      "Epoch 10/25\n",
      "136770/136770 [==============================] - 82s 600us/sample - loss: 0.0962 - val_loss: 0.1533\n",
      "Epoch 11/25\n",
      "136770/136770 [==============================] - 82s 598us/sample - loss: 0.0949 - val_loss: 0.1500\n",
      "Epoch 12/25\n",
      "136770/136770 [==============================] - 82s 599us/sample - loss: 0.0940 - val_loss: 0.1482\n",
      "Epoch 13/25\n",
      "136770/136770 [==============================] - 82s 598us/sample - loss: 0.0931 - val_loss: 0.1501\n",
      "Epoch 14/25\n",
      "136770/136770 [==============================] - 83s 610us/sample - loss: 0.0921 - val_loss: 0.1521\n",
      "Epoch 15/25\n",
      "136770/136770 [==============================] - 82s 596us/sample - loss: 0.0910 - val_loss: 0.1514\n",
      "Epoch 16/25\n",
      "136770/136770 [==============================] - 82s 598us/sample - loss: 0.0901 - val_loss: 0.1472\n",
      "Epoch 17/25\n",
      "136770/136770 [==============================] - 82s 598us/sample - loss: 0.0892 - val_loss: 0.1531\n",
      "Epoch 18/25\n",
      "136770/136770 [==============================] - 82s 603us/sample - loss: 0.0883 - val_loss: 0.1468\n",
      "Epoch 19/25\n",
      "136770/136770 [==============================] - 82s 602us/sample - loss: 0.0874 - val_loss: 0.1509\n",
      "Epoch 20/25\n",
      "136770/136770 [==============================] - 82s 602us/sample - loss: 0.0865 - val_loss: 0.1528\n",
      "Epoch 21/25\n",
      "136770/136770 [==============================] - 82s 600us/sample - loss: 0.0856 - val_loss: 0.1519\n",
      "Epoch 22/25\n",
      "136770/136770 [==============================] - 82s 602us/sample - loss: 0.0848 - val_loss: 0.1510\n",
      "Epoch 23/25\n",
      "136770/136770 [==============================] - 82s 601us/sample - loss: 0.0839 - val_loss: 0.1490\n",
      "Epoch 24/25\n",
      " 54464/136770 [==========>...................] - ETA: 49s - loss: 0.0822"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-07a92f19a4c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m     history = model.fit(x=[trainingData_time, trainingData_static], y=trainingAns_time,\n\u001b[0;32m     41\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestingData_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestingData_static\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestingAns_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m               epochs=num_epoch, shuffle=True, batch_size=64)\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mmodelList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mhistoryList512\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inputSize = 7\n",
    "normalizeTarget = False\n",
    "\n",
    "quantile = 0.5\n",
    "num_epoch = 25\n",
    "\n",
    "NUM_CELLS = 512\n",
    "NUM_DAYS_OUT = 14\n",
    "\n",
    "num_repeat = 2\n",
    "modelList = []\n",
    "historyList512 = []\n",
    "\n",
    "\n",
    "# normalization - static\n",
    "data_static_zscore, data_static_mean, data_static_std = normalizeData(dataList_static_PCA)\n",
    "\n",
    "# normalization - time\n",
    "data_time_zscore, data_time_mean, data_time_std = normalizeData(dataList_time)\n",
    "\n",
    "# for static dataset\n",
    "trainingData_static, testingData_static = dataPrep_LSTM_fromNP_static(data_static_zscore, data_time_zscore, inputSize, 14, 1, 14)\n",
    "\n",
    "# for timeseries dataset\n",
    "trainingData_time, trainingAns_time, testingData_time, testingAns_time = dataPrep_LSTM_fromNP_time(data_time_zscore, 'deaths', inputSize, 14, 1, 14, columns_time_selected_reorder)\n",
    "\n",
    "# unnormalize the targets\n",
    "if not normalizeTarget:\n",
    "    trainingAns_time, testingAns_time = unnormalizeTarget(trainingAns_time, testingAns_time, data_time_mean, data_time_std, columns_time_selected_reorder, 'deaths')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# train the model\n",
    "for rep in range(num_repeat):\n",
    "    model = condLSTM(NUM_CELLS, NUM_DAYS_OUT)\n",
    "    model.call([trainingData_time, trainingData_static])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss=lambda y_p, y: quantileLoss(quantile, y_p, y))\n",
    "    history = model.fit(x=[trainingData_time, trainingData_static], y=trainingAns_time,\n",
    "              validation_data=([testingData_time, testingData_static], testingAns_time),\n",
    "              epochs=num_epoch, shuffle=True, batch_size=64)\n",
    "    modelList.append(model)\n",
    "    historyList512.append(history)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_loss = np.array( [ x.history['val_loss'] for x in historyList ] )\n",
    "print(val_loss.shape)\n",
    "val_loss_avg = np.mean(val_loss, axis = 0)\n",
    "\n",
    "plt.plot(val_loss_avg)\n",
    "plt.ylim(0.14, 0.20)\n",
    "#plt.title('Removed %d features from time dataset' % (numRemove_static_list[i_static]))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimize the number of epochs for each quantile\n",
    "inputSize = 7\n",
    "normalizeTarget = False\n",
    "\n",
    "quantileList = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "num_epoch = 10\n",
    "\n",
    "NUM_CELLS = 512\n",
    "NUM_DAYS_OUT = 14\n",
    "\n",
    "num_repeat = 2\n",
    "modelList = []\n",
    "historyList512 = []\n",
    "\n",
    "\n",
    "# normalization - static\n",
    "data_static_zscore, data_static_mean, data_static_std = normalizeData(dataList_static_PCA)\n",
    "\n",
    "# normalization - time\n",
    "data_time_zscore, data_time_mean, data_time_std = normalizeData(dataList_time)\n",
    "\n",
    "# for static dataset\n",
    "trainingData_static, testingData_static = dataPrep_LSTM_fromNP_static(data_static_zscore, data_time_zscore, inputSize, 14, 1, 14)\n",
    "\n",
    "# for timeseries dataset\n",
    "trainingData_time, trainingAns_time, testingData_time, testingAns_time = dataPrep_LSTM_fromNP_time(data_time_zscore, 'deaths', inputSize, 14, 1, 14, columns_time_selected_reorder)\n",
    "\n",
    "# unnormalize the targets\n",
    "if not normalizeTarget:\n",
    "    trainingAns_time, testingAns_time = unnormalizeTarget(trainingAns_time, testingAns_time, data_time_mean, data_time_std, columns_time_selected_reorder, 'deaths')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# train the model\n",
    "for rep in range(num_repeat):\n",
    "    model = condLSTM(NUM_CELLS, NUM_DAYS_OUT)\n",
    "    model.call([trainingData_time, trainingData_static])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss=lambda y_p, y: quantileLoss(quantile, y_p, y))\n",
    "    history = model.fit(x=[trainingData_time, trainingData_static], y=trainingAns_time,\n",
    "              validation_data=([testingData_time, testingData_static], testingAns_time),\n",
    "              epochs=num_epoch, shuffle=True, batch_size=64)\n",
    "    modelList.append(model)\n",
    "    historyList512.append(history)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
